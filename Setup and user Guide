To fetch Twitter data, we will have to follow the steps given below −
1.Create a twitter Application
2.Install / Start HDFS
3.Configure Flume

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Creating a Twitter Application
In order to get the tweets from Twitter, it is needed to create a Twitter application. 
Follow the steps given below to create a Twitter application.
-------------------------------------------------------------------------------------------------------------------------------------
Step 1
To create a Twitter application, click on the following link https://apps.twitter.com/. 
Sign in to your Twitter account. You will have a Twitter Application Management window
where you can create, delete, and manage Twitter Apps.
--------------------------------------------------------------------------------------------------------------------------------------------------
Step 2
Click on the Create New App button. You will be redirected to a window where you will 
get an application form in which you have to fill in your details in order to create the
App. While filling the website address, give the complete URL pattern, for example, http://example.com. 
NOTE( it is not necessary to have a website you can give  any URL)
---------------------------------------------------------------------------------------------------------------------------------------
Step 3
Fill in the details, accept the Developer Agreement when finished, click on the Create your Twitter
application button which is at the bottom of the page. If everything goes fine, an App will be created 
with the given details.
---------------------------------------------------------------------------------------------------------------------------------------
Step 4
Under keys and Access Tokens tab at the bottom of the page, you can observe a button named Create my
access token. Click on it to generate the access token.
Here the work is complete. now let us go to hdfs 
----------------------------------------------------------------------------------------------------------------------------------
Step 1 start HDFS 
Start-all.sh or (start-dfs.sh and start-yarn.sh)
step 2 -create an hdfs directory 
hdfs dfs -ls mkdir /user/hadoop/twitter_data 
----------------------------------------------------------------------------------------------------------------------------------
Configuring Flume
We have to configure the source, the channel, and the sink using the configuration file in the conf folder

Setting the classpath
Set the classpath variable to the lib folder of Flume in Flume-env.sh file as shown below.

export CLASSPATH=$CLASSPATH:/FLUME_HOME/lib/* 


This source needs the details such as Consumer key, Consumer secret, Access token, 
and Access token secret of a Twitter application. While configuring this source, you 
have to provide values to the following properties −

Channels

Source type : org.apache.flume.source.twitter.TwitterSource

consumerKey − The OAuth consumer key

consumerSecret − OAuth consumer secret

accessToken − OAuth access token

accessTokenSecret − OAuth token secret

maxBatchSize − Maximum number of twitter messages that should be in a twitter batch. The default value is 1000 (optional).

maxBatchDurationMillis − Maximum number of milliseconds to wait before closing a batch. The default value is 1000 (optional).

Channel
We are using the memory channel. To configure the memory channel, you must provide value to the type of the channel.

type − It holds the type of the channel. In our example, the type is MemChannel.

Capacity − It is the maximum number of events stored in the channel. Its default value is 100 (optional).

TransactionCapacity − It is the maximum number of events the channel accepts or sends. Its default value is 100 (optional).

HDFS Sink
This sink writes data into the HDFS. To configure this sink, you must provide the following details.

Channel

type − hdfs

hdfs.path − the path of the directory in HDFS where data is to be stored.

And we can provide some optional values based on the scenario. Given below are the optional properties of the HDFS sink that we are configuring in our application.

fileType − This is the required file format of our HDFS file. SequenceFile, DataStream and CompressedStream are the three types available with this stream. In our example, we are using the DataStream.

writeFormat − Could be either text or writable.

batchSize − It is the number of events written to a file before it is flushed into the HDFS. Its default value is 100.

rollsize − It is the file size to trigger a roll. It default value is 100.

rollCount − It is the number of events written into the file before it is rolled. Its default value is 10.


FOR EG flume conf FILE 
# Naming the components on the current agent. 
TwitterAgent.sources = Twitter 
TwitterAgent.channels = MemChannel 
TwitterAgent.sinks = HDFS
  
# Describing/Configuring the source 
TwitterAgent.sources.Twitter.type = org.apache.flume.source.twitter.TwitterSource
TwitterAgent.sources.Twitter.consumerKey = Your OAuth consumer key
TwitterAgent.sources.Twitter.consumerSecret = Your OAuth consumer secret 
TwitterAgent.sources.Twitter.accessToken = Your OAuth consumer key access token 
TwitterAgent.sources.Twitter.accessTokenSecret = Your OAuth consumer key access token secret 
TwitterAgent.sources.Twitter.keywords = tutorials point,java, bigdata, mapreduce, mahout, hbase, nosql
  
# Describing/Configuring the sink 

TwitterAgent.sinks.HDFS.type = hdfs 
TwitterAgent.sinks.HDFS.hdfs.path = hdfs://localhost:9000/user/Hadoop/twitter_data/
TwitterAgent.sinks.HDFS.hdfs.fileType = DataStream 
TwitterAgent.sinks.HDFS.hdfs.writeFormat = Text 
TwitterAgent.sinks.HDFS.hdfs.batchSize = 1000
TwitterAgent.sinks.HDFS.hdfs.rollSize = 0 
TwitterAgent.sinks.HDFS.hdfs.rollCount = 10000 
 
# Describing/Configuring the channel 
TwitterAgent.channels.MemChannel.type = memory 
TwitterAgent.channels.MemChannel.capacity = 10000 
TwitterAgent.channels.MemChannel.transactionCapacity = 100
  
# Binding the source and sink to the channel 
TwitterAgent.sources.Twitter.channels = MemChannel
TwitterAgent.sinks.HDFS.channel = MemChannel

-------------------------------------------------------------------------------------------------------------------------------------------------
Execution
Browse through the Flume home directory and execute the application as shown below.

$ cd $FLUME_HOME 
$ bin/flume-ng agent --conf ./conf/ -f conf/twitter.conf  Dflume.root.logger=DEBUG,console -n TwitterAgent

-------------------------------------------------------------------------------------------------------------------------------------------------------------
Verifying HDFS
You can access the Hadoop Administration Web UI using the URL given below.

http://localhost:50070/ 
